{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PU Learning For News\n",
    "Initial work\n",
    "\n",
    "Mainly to start exploring how to vectorize texts and also identify true negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atin3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atin3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\atin3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>house dem aide didnt even see comeys letter ja...</td>\n",
       "      <td>house dem aide didnt even see comeys letter ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>ever get feeling life circle roundabout rather...</td>\n",
       "      <td>flynn hillary clinton big woman campus breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>truth might get fired october tension intellig...</td>\n",
       "      <td>truth might get fired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>video civilian killed single u airstrike ident...</td>\n",
       "      <td>civilian killed single u airstrike identified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>print iranian woman sentenced six year prison ...</td>\n",
       "      <td>iranian woman jailed fictional unpublished sto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  house dem aide didnt even see comeys letter ja...   \n",
       "1  ever get feeling life circle roundabout rather...   \n",
       "2  truth might get fired october tension intellig...   \n",
       "3  video civilian killed single u airstrike ident...   \n",
       "4  print iranian woman sentenced six year prison ...   \n",
       "\n",
       "                                         clean_title  \n",
       "0  house dem aide didnt even see comeys letter ja...  \n",
       "1   flynn hillary clinton big woman campus breitbart  \n",
       "2                              truth might get fired  \n",
       "3      civilian killed single u airstrike identified  \n",
       "4  iranian woman jailed fictional unpublished sto...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "input_file = \"./article_dataset.csv\"\n",
    "df_raw = pd.read_csv(input_file, index_col=\"id\")\n",
    "df_raw = preprocessing.clean_df(df_raw)\n",
    "df_raw = df_raw[df_raw['cleaned_text'].str.strip() != \"\"].copy()\n",
    "df_raw.reset_index(inplace=True)\n",
    "df_raw['id'] = df_raw.index\n",
    "df_raw[\"clean_title\"] = df_raw['title'].apply(preprocessing.clean_text)\n",
    "\n",
    "df = df_raw.copy()\n",
    "df = preprocessing.turn_into_pu(df, 0.5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocessing.test_train_split(df)\n",
    "y_train.index = X_train.index\n",
    "y_test_true = df_raw.loc[y_test.index, 'label']\n",
    "\n",
    "X_train_bootstrap, y_train_bootstrap = resample(\n",
    "    X_train, y_train,\n",
    "    replace=True,\n",
    "    n_samples=len(X_train),\n",
    "    random_state=42\n",
    ")\n",
    "X_train, y_train = X_train_bootstrap, y_train_bootstrap\n",
    "\n",
    "X_train['tokens'] = X_train['cleaned_text'].apply(lambda x: x.split())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Text\n",
    "TF-IDF words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance:  [0.01367917 0.02235335 0.02860911 0.03448028 0.03940357 0.04338159]\n",
      "Time elapsed: 254.11 seconds\n",
      "Explained Variance:  [0.01367917 0.02235335 0.02860911 0.03448028 0.03940363 0.0433828\n",
      " 0.04717504 0.05065785 0.0538754  0.05682713 0.05948698 0.06209655\n",
      " 0.06463824 0.06711149 0.06955993 0.07194289 0.07418611 0.07634686\n",
      " 0.07844643 0.08047327 0.08240216 0.08429703 0.08618565 0.08800001\n",
      " 0.08977545 0.09149533 0.09320106 0.0948481  0.09645029 0.09802663]\n",
      "Time elapsed: 575.50 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=True)\n",
    "tfidf_matrix = vectorizer.fit_transform(X_train['cleaned_text'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=X_train.index)\n",
    "\n",
    "pca_logistic = PCA(n_components=6)\n",
    "principal_components_logistic = pca_logistic.fit_transform(tfidf_df)\n",
    "print(\"Explained Variance: \", pca_logistic.explained_variance_ratio_.cumsum())\n",
    "end_time = time.time()\n",
    "print(\"Time elapsed: {:.2f} seconds\".format(end_time - start_time))\n",
    "\n",
    "pca_svm = PCA(n_components=30)\n",
    "principal_components_svm = pca_svm.fit_transform(tfidf_df)\n",
    "print(\"Explained Variance: \", pca_svm.explained_variance_ratio_.cumsum())\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time elapsed: {:.2f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing through Document Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 449.32 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_maker(df: pd.DataFrame, type:str):\n",
    "    df['sentiment'] = df[type].apply(analyzer.polarity_scores)\n",
    "    df = pd.concat([df.drop(['sentiment'], axis=1), df['sentiment'].apply(pd.Series)], axis=1)\n",
    "    df.rename(columns={col: f\"{col}_{type}\" for col in [\"pos\", \"neu\", \"neg\"]}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def count_stop_words(tokens):\n",
    "    stop_word_count = sum(1 for word in tokens if word in stop_words)\n",
    "    return stop_word_count / len(tokens)\n",
    "\n",
    "X_vec = X_train.copy()\n",
    "X_vec['len'] = X_vec['tokens'].apply(lambda x: len(x))\n",
    "\n",
    "X_vec = sentiment_maker(X_vec, \"cleaned_text\")\n",
    "X_vec = sentiment_maker(X_vec, \"clean_title\")\n",
    "\n",
    "X_vec['percent_stop'] = X_vec['tokens'].apply(count_stop_words)\n",
    "X_vec\n",
    "end_time = time.time()\n",
    "print(\"Time elapsed: {:.2f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_combined(X_vec, principal_components):\n",
    "    X_vec_numeric = X_vec.select_dtypes(include=np.number).drop(columns=[\"id\"])\n",
    "    principal_components_df = pd.DataFrame(principal_components)\n",
    "    temp_index = X_vec.index\n",
    "    X_vec_numeric.reset_index(inplace=True)\n",
    "    principal_components_df.reset_index(inplace=True)\n",
    "    x_train_combined = pd.concat([X_vec_numeric, principal_components_df], axis=1)\n",
    "    x_train_combined.drop(columns=['index'], inplace=True)\n",
    "    x_train_combined.set_index(temp_index, inplace=True)\n",
    "    x_train_combined.columns = x_train_combined.columns.astype(str)\n",
    "    return x_train_combined\n",
    "\n",
    "x_train_combined_logistic = get_train_combined(X_vec, principal_components_logistic)\n",
    "x_train_combined_svm = get_train_combined(X_vec, principal_components_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing X test\n",
    "## Same preprocessing X train went through\n",
    "## Utilizing fitted pca and tfidf\n",
    "test_tfidf_matrix = vectorizer.transform(X_test['cleaned_text'])\n",
    "\n",
    "test_tfidf = pd.DataFrame(test_tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=X_test.index)\n",
    "test_pca_logistic = pd.DataFrame(pca_logistic.transform(test_tfidf))\n",
    "test_pca_svm = pd.DataFrame(pca_svm.transform(test_tfidf))\n",
    "\n",
    "X_test_vec = X_test.copy()\n",
    "X_test_vec['tokens'] = X_test_vec['cleaned_text'].apply(lambda x: x.split())\n",
    "X_test_vec['len'] = X_test_vec['tokens'].apply(lambda x: len(x))\n",
    "X_test_vec = sentiment_maker(X_test_vec, \"cleaned_text\")\n",
    "X_test_vec = sentiment_maker(X_test_vec, \"clean_title\")\n",
    "X_test_vec['percent_stop'] = X_test_vec['tokens'].apply(count_stop_words)\n",
    "\n",
    "X_vec_numeric = X_test_vec.select_dtypes(include=np.number).drop(columns=[\"id\"])\n",
    "x_test_index = X_test.index\n",
    "X_vec_numeric.reset_index(inplace=True)\n",
    "test_pca_logistic.reset_index(inplace=True)\n",
    "test_pca_svm.reset_index(inplace=True)\n",
    "\n",
    "x_test_processed_logistic = pd.concat([X_vec_numeric, test_pca_logistic], axis=1)\n",
    "x_test_processed_logistic.set_index(x_test_index, inplace=True)\n",
    "x_test_processed_logistic.columns = x_test_processed_logistic.columns.astype(str)\n",
    "x_test_processed_logistic.drop(columns=['index'], inplace=True)\n",
    "\n",
    "x_test_processed_svm = pd.concat([X_vec_numeric, test_pca_svm], axis=1)\n",
    "x_test_processed_svm.set_index(x_test_index, inplace=True)\n",
    "x_test_processed_svm.columns = x_test_processed_svm.columns.astype(str)\n",
    "x_test_processed_svm.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parmeter tuning for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 660 candidates, totalling 1980 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "891 fits failed out of a total of 1980.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "99 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.75524374        nan 0.75524374 0.75321859\n",
      " 0.75321859 0.75524374 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75524374        nan\n",
      " 0.75524374 0.75321859 0.75321859 0.75524374 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75524374        nan 0.75524374 0.75321859 0.75321859 0.75524374\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.75524374        nan 0.75524374 0.75329091\n",
      " 0.75329091 0.75524374 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75524374        nan\n",
      " 0.75524374 0.75329091 0.75329091 0.75524374 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75524374        nan 0.75524374 0.75329091 0.75329091 0.75524374\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.75524374        nan 0.75524374 0.75372486\n",
      " 0.75372486 0.75524374 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75524374        nan\n",
      " 0.75524374 0.75372486 0.75372486 0.75524374 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75524374        nan 0.75524374 0.75372486 0.75372486 0.75524374\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.7537972         nan 0.75524374 0.75379722\n",
      " 0.7537249  0.75430347 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.7537972         nan\n",
      " 0.75524374 0.75379722 0.7537249  0.75430347 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.7537972         nan 0.75524374 0.75379722 0.7537249  0.75430347\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.75524363        nan 0.75524374 0.75531602\n",
      " 0.75524368 0.75560531 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75517131        nan\n",
      " 0.75524374 0.75531602 0.75524368 0.75560531 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75495434        nan 0.75524374 0.75531602 0.75524368 0.75560531\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.75437581        nan 0.75524374 0.75618388\n",
      " 0.75669012 0.75647314 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75459283        nan\n",
      " 0.75524374 0.75618388 0.75669012 0.75647314 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75444816        nan 0.75524374 0.75618388 0.75669012 0.75647314\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.75423117        nan 0.75524374 0.75546072\n",
      " 0.75546072 0.7564732  0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75415884        nan\n",
      " 0.75524374 0.75546072 0.75546072 0.7564732  0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75401418        nan 0.75524374 0.75546072 0.75546072 0.7564732\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.75314629        nan 0.75524374 0.75437597\n",
      " 0.7546651  0.75546079 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75300166        nan\n",
      " 0.75524374 0.75437597 0.7546651  0.75546079 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75307396        nan 0.75524374 0.75437597 0.7546651  0.75546079\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.7531463         nan 0.75524374 0.75480993\n",
      " 0.75358019 0.75452053 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75307396        nan\n",
      " 0.75524374 0.75480993 0.75358019 0.75452053 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75314632        nan 0.75524374 0.75480993 0.75358019 0.75452053\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.75292933        nan 0.75524374 0.75437603\n",
      " 0.75307401 0.75473753 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75292932        nan\n",
      " 0.75524374 0.75437603 0.75307401 0.75473753 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75285698        nan 0.75524374 0.75437603 0.75307401 0.75473753\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374\n",
      "        nan        nan 0.75292932        nan 0.75524374 0.75560551\n",
      " 0.75329091 0.75473755 0.75524374 0.75524374        nan        nan\n",
      "        nan        nan        nan 0.75473772 0.75307393        nan\n",
      " 0.75524374 0.75524374        nan        nan 0.75264001        nan\n",
      " 0.75524374 0.75560551 0.75329091 0.75473755 0.75524374 0.75524374\n",
      "        nan        nan        nan        nan        nan 0.75473772\n",
      " 0.75307393        nan 0.75524374 0.75524374        nan        nan\n",
      " 0.75264003        nan 0.75524374 0.75560551 0.75329091 0.75473755\n",
      " 0.75524374 0.75524374        nan        nan        nan        nan\n",
      "        nan 0.75473772 0.75307393        nan 0.75524374 0.75524374]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [    \n",
    "    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "    'max_iter' : [1000,2500, 5000]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(logit, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)\n",
    "best_clf = clf.fit(x_train_combined_logistic,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Logistic Regression Model\n",
    "Model trained on PU data\\\n",
    "Confusion Matrix based on true values of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Making Model\n",
    "logreg = LogisticRegression(random_state=97)\n",
    "logreg.fit(x_train_combined_logistic, y_train)\n",
    "\n",
    "y_pred_proba = logreg.predict_proba(x_test_processed_logistic)\n",
    "y_pred_logistic = [1 if proba[1] > 0.8 else 0 for proba in y_pred_proba]\n",
    "confusion_matrix = metrics.confusion_matrix(y_test_true, y_pred_logistic)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>neg_cleaned_text</th>\n",
       "      <th>neu_cleaned_text</th>\n",
       "      <th>pos_cleaned_text</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg_clean_title</th>\n",
       "      <th>neu_clean_title</th>\n",
       "      <th>pos_clean_title</th>\n",
       "      <th>compound</th>\n",
       "      <th>percent_stop</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17038</th>\n",
       "      <td>125</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.4348</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020961</td>\n",
       "      <td>-0.055251</td>\n",
       "      <td>-0.007201</td>\n",
       "      <td>0.038184</td>\n",
       "      <td>-0.005241</td>\n",
       "      <td>-0.033393</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>-0.014820</td>\n",
       "      <td>0.004429</td>\n",
       "      <td>-0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>529</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.015123</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024423</td>\n",
       "      <td>0.060845</td>\n",
       "      <td>0.030136</td>\n",
       "      <td>-0.046778</td>\n",
       "      <td>-0.078419</td>\n",
       "      <td>0.084837</td>\n",
       "      <td>0.036675</td>\n",
       "      <td>0.021714</td>\n",
       "      <td>-0.047267</td>\n",
       "      <td>-0.024534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>742</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.9981</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.014276</td>\n",
       "      <td>0.026971</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>-0.061707</td>\n",
       "      <td>-0.064360</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>-0.048074</td>\n",
       "      <td>-0.105336</td>\n",
       "      <td>0.190910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>675</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.9646</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>-0.050312</td>\n",
       "      <td>-0.001253</td>\n",
       "      <td>0.037205</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.014762</td>\n",
       "      <td>-0.011616</td>\n",
       "      <td>-0.032474</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>-0.036927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13379</th>\n",
       "      <td>392</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049265</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>-0.073598</td>\n",
       "      <td>-0.011675</td>\n",
       "      <td>-0.023894</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.059913</td>\n",
       "      <td>-0.034842</td>\n",
       "      <td>0.064052</td>\n",
       "      <td>0.074590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8325</th>\n",
       "      <td>362</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031509</td>\n",
       "      <td>-0.019638</td>\n",
       "      <td>0.004429</td>\n",
       "      <td>0.030320</td>\n",
       "      <td>-0.000383</td>\n",
       "      <td>-0.013876</td>\n",
       "      <td>0.029988</td>\n",
       "      <td>-0.013807</td>\n",
       "      <td>-0.007683</td>\n",
       "      <td>0.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8973</th>\n",
       "      <td>460</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.9968</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008623</td>\n",
       "      <td>-0.023980</td>\n",
       "      <td>-0.020179</td>\n",
       "      <td>0.011508</td>\n",
       "      <td>-0.010571</td>\n",
       "      <td>-0.004374</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>-0.013622</td>\n",
       "      <td>-0.005086</td>\n",
       "      <td>-0.021405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>126</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.8470</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018723</td>\n",
       "      <td>0.016838</td>\n",
       "      <td>-0.009732</td>\n",
       "      <td>-0.004647</td>\n",
       "      <td>0.007412</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>-0.017697</td>\n",
       "      <td>-0.022624</td>\n",
       "      <td>0.035041</td>\n",
       "      <td>0.019419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9319</th>\n",
       "      <td>161</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.9806</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054544</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.032703</td>\n",
       "      <td>-0.028751</td>\n",
       "      <td>-0.018230</td>\n",
       "      <td>-0.021746</td>\n",
       "      <td>-0.018346</td>\n",
       "      <td>-0.029811</td>\n",
       "      <td>-0.001469</td>\n",
       "      <td>0.000547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>639</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.7666</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.7351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058710</td>\n",
       "      <td>-0.021307</td>\n",
       "      <td>0.035947</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>-0.028424</td>\n",
       "      <td>-0.017037</td>\n",
       "      <td>-0.040277</td>\n",
       "      <td>-0.038698</td>\n",
       "      <td>-0.044813</td>\n",
       "      <td>-0.016273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6810 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       len  neg_cleaned_text  neu_cleaned_text  pos_cleaned_text  compound  \\\n",
       "17038  125             0.071             0.871             0.057   -0.4348   \n",
       "7623   529             0.047             0.781             0.172    0.9965   \n",
       "1647   742             0.196             0.706             0.097   -0.9981   \n",
       "4727   675             0.137             0.742             0.122   -0.9646   \n",
       "13379  392             0.036             0.773             0.191    0.9958   \n",
       "...    ...               ...               ...               ...       ...   \n",
       "8325   362             0.041             0.719             0.240    0.9979   \n",
       "8973   460             0.200             0.701             0.099   -0.9968   \n",
       "6242   126             0.109             0.709             0.182    0.8470   \n",
       "9319   161             0.212             0.737             0.051   -0.9806   \n",
       "2352   639             0.100             0.780             0.120    0.7666   \n",
       "\n",
       "       neg_clean_title  neu_clean_title  pos_clean_title  compound  \\\n",
       "17038            0.000            1.000            0.000    0.0000   \n",
       "7623             0.000            0.777            0.223    0.3182   \n",
       "1647             0.000            1.000            0.000    0.0000   \n",
       "4727             0.248            0.537            0.215   -0.1280   \n",
       "13379            0.000            1.000            0.000    0.0000   \n",
       "...                ...              ...              ...       ...   \n",
       "8325             0.000            0.795            0.205    0.2023   \n",
       "8973             0.301            0.699            0.000   -0.6486   \n",
       "6242             0.000            1.000            0.000    0.0000   \n",
       "9319             0.273            0.727            0.000   -0.6705   \n",
       "2352             0.508            0.492            0.000   -0.7351   \n",
       "\n",
       "       percent_stop  ...        20        21        22        23        24  \\\n",
       "17038      0.000000  ... -0.020961 -0.055251 -0.007201  0.038184 -0.005241   \n",
       "7623       0.015123  ... -0.024423  0.060845  0.030136 -0.046778 -0.078419   \n",
       "1647       0.000000  ...  0.018115  0.014276  0.026971  0.003505 -0.061707   \n",
       "4727       0.002963  ...  0.004409 -0.050312 -0.001253  0.037205  0.000113   \n",
       "13379      0.005102  ... -0.049265 -0.021509 -0.073598 -0.011675 -0.023894   \n",
       "...             ...  ...       ...       ...       ...       ...       ...   \n",
       "8325       0.005525  ...  0.031509 -0.019638  0.004429  0.030320 -0.000383   \n",
       "8973       0.000000  ... -0.008623 -0.023980 -0.020179  0.011508 -0.010571   \n",
       "6242       0.007937  ...  0.018723  0.016838 -0.009732 -0.004647  0.007412   \n",
       "9319       0.000000  ...  0.054544  0.031841  0.032703 -0.028751 -0.018230   \n",
       "2352       0.000000  ...  0.058710 -0.021307  0.035947  0.039837 -0.028424   \n",
       "\n",
       "             25        26        27        28        29  \n",
       "17038 -0.033393  0.009406 -0.014820  0.004429 -0.002115  \n",
       "7623   0.084837  0.036675  0.021714 -0.047267 -0.024534  \n",
       "1647  -0.064360  0.007998 -0.048074 -0.105336  0.190910  \n",
       "4727  -0.014762 -0.011616 -0.032474  0.007357 -0.036927  \n",
       "13379 -0.000103 -0.059913 -0.034842  0.064052  0.074590  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "8325  -0.013876  0.029988 -0.013807 -0.007683  0.009496  \n",
       "8973  -0.004374  0.004292 -0.013622 -0.005086 -0.021405  \n",
       "6242   0.005370 -0.017697 -0.022624  0.035041  0.019419  \n",
       "9319  -0.021746 -0.018346 -0.029811 -0.001469  0.000547  \n",
       "2352  -0.017037 -0.040277 -0.038698 -0.044813 -0.016273  \n",
       "\n",
       "[6810 rows x 40 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_processed_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_combined_svm)\n",
    "x_test_scaled = scaler.transform(x_test_processed_svm)\n",
    "svm = SVC(kernel=\"linear\", C=1.0, gamma=\"scale\", probability=True, random_state=100)\n",
    "svm.fit(x_train_scaled, y_train)\n",
    "\n",
    "# y_pred = svm.predict(x_test_scaled)\n",
    "y_probs = svm.predict_proba(x_test_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at Threshold: 0.8\n",
      " Accuracy: 0.818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      3477\n",
      "           1       0.82      0.80      0.81      3333\n",
      "\n",
      "    accuracy                           0.82      6810\n",
      "   macro avg       0.82      0.82      0.82      6810\n",
      "weighted avg       0.82      0.82      0.82      6810\n",
      "\n",
      "y_test distribution: [3477 3333]\n",
      "y_pred distribution: [3559 3251]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "threshold = 0.8\n",
    "y_pred_svm = (y_probs >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test_true, y_pred_svm)\n",
    "\n",
    "print(f\"Evaluation at Threshold: {threshold}\")\n",
    "print(f\" Accuracy: {accuracy:.3f}\")\n",
    "print(classification_report(y_test_true, y_pred_svm))\n",
    "print(\"y_test distribution:\", np.bincount(y_test_true))\n",
    "print(\"y_pred distribution:\", np.bincount(y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>tokens</th>\n",
       "      <th>neg_cleaned_text</th>\n",
       "      <th>neu_cleaned_text</th>\n",
       "      <th>pos_cleaned_text</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg_clean_title</th>\n",
       "      <th>neu_clean_title</th>\n",
       "      <th>pos_clean_title</th>\n",
       "      <th>compound</th>\n",
       "      <th>percent_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17038</th>\n",
       "      <td>17038</td>\n",
       "      <td>McCain: ‘I Don’t Know’ If Trump-Russia Dossier...</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>Wednesday on CNN addressing media reports that...</td>\n",
       "      <td>wednesday cnn addressing medium report turned ...</td>\n",
       "      <td>mccain dont know trumprussia dossier credible ...</td>\n",
       "      <td>[wednesday, cnn, addressing, medium, report, t...</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.4348</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>7623</td>\n",
       "      <td>Netflix Casts a Wider Net for Original Documen...</td>\n",
       "      <td>Glenn Kenny</td>\n",
       "      <td>It was early in March 2015, the filmmaker Ava ...</td>\n",
       "      <td>early march filmmaker ava duvernay recalled re...</td>\n",
       "      <td>netflix cast wider net original documentary ne...</td>\n",
       "      <td>[early, march, filmmaker, ava, duvernay, recal...</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.015123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>1647</td>\n",
       "      <td>Aleppo Evacuation Effort Restarts, and Assad C...</td>\n",
       "      <td>Ben Hubbard and Hwaida Saad</td>\n",
       "      <td>BEIRUT, Lebanon  —   After months of fierce bo...</td>\n",
       "      <td>beirut lebanon month fierce bombardment failed...</td>\n",
       "      <td>aleppo evacuation effort restarts assad call h...</td>\n",
       "      <td>[beirut, lebanon, month, fierce, bombardment, ...</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.9981</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>4727</td>\n",
       "      <td>Friends Say Minnesota Attacker Was ‘Normal Ame...</td>\n",
       "      <td>Mitch Smith and Richard Pérez-Peña</td>\n",
       "      <td>ST. CLOUD, Minn.  —   The man who the police s...</td>\n",
       "      <td>st cloud minn man police say stabbed people ma...</td>\n",
       "      <td>friend say minnesota attacker normal american ...</td>\n",
       "      <td>[st, cloud, minn, man, police, say, stabbed, p...</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.9646</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>0.002963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13379</th>\n",
       "      <td>13379</td>\n",
       "      <td>Nearly Four-Fifths of White Evangelicals Say T...</td>\n",
       "      <td>Laurie Goodstein</td>\n",
       "      <td>Nearly   of white evangelical voters plan to c...</td>\n",
       "      <td>nearly white evangelical voter plan cast ballo...</td>\n",
       "      <td>nearly fourfifths white evangelicals say theyl...</td>\n",
       "      <td>[nearly, white, evangelical, voter, plan, cast...</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8325</th>\n",
       "      <td>8325</td>\n",
       "      <td>When a Basic Laptop Will Do: Our Top Budget Pi...</td>\n",
       "      <td>Damon Darlin</td>\n",
       "      <td>Ever since the advent of the iPhone and iPad, ...</td>\n",
       "      <td>ever since advent iphone ipad people using lap...</td>\n",
       "      <td>basic laptop top budget pick new york time</td>\n",
       "      <td>[ever, since, advent, iphone, ipad, people, us...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.005525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8973</th>\n",
       "      <td>8973</td>\n",
       "      <td>Leader of a Ku Klux Klan Group Is Found Dead i...</td>\n",
       "      <td>Liam Stack</td>\n",
       "      <td>Frank Ancona, the professed leader of the Trad...</td>\n",
       "      <td>frank ancona professed leader traditionalist a...</td>\n",
       "      <td>leader ku klux klan group found dead missouri ...</td>\n",
       "      <td>[frank, ancona, professed, leader, traditional...</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.9968</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6486</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>6242</td>\n",
       "      <td>Economic Breadth Is Significantly Deterioratin...</td>\n",
       "      <td>IWB</td>\n",
       "      <td>Economic Breadth Is Significantly Deterioratin...</td>\n",
       "      <td>economic breadth significantly deteriorating u...</td>\n",
       "      <td>economic breadth significantly deteriorating u</td>\n",
       "      <td>[economic, breadth, significantly, deteriorati...</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.8470</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9319</th>\n",
       "      <td>9319</td>\n",
       "      <td>Keith Lamont Scott Was Killed by Two Gunshot W...</td>\n",
       "      <td>Niraj Chokshi</td>\n",
       "      <td>A    black man whose fatal shooting by the pol...</td>\n",
       "      <td>black man whose fatal shooting police septembe...</td>\n",
       "      <td>keith lamont scott killed two gunshot wound fa...</td>\n",
       "      <td>[black, man, whose, fatal, shooting, police, s...</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.9806</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6705</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>2352</td>\n",
       "      <td>A Fatality Forces Tesla to Confront Its Limits...</td>\n",
       "      <td>Bill Vlasic</td>\n",
       "      <td>DETROIT  —   As the   and chief executive of T...</td>\n",
       "      <td>detroit chief executive tesla motor technology...</td>\n",
       "      <td>fatality force tesla confront limit new york time</td>\n",
       "      <td>[detroit, chief, executive, tesla, motor, tech...</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.7666</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.7351</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6810 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "17038  17038  McCain: ‘I Don’t Know’ If Trump-Russia Dossier...   \n",
       "7623    7623  Netflix Casts a Wider Net for Original Documen...   \n",
       "1647    1647  Aleppo Evacuation Effort Restarts, and Assad C...   \n",
       "4727    4727  Friends Say Minnesota Attacker Was ‘Normal Ame...   \n",
       "13379  13379  Nearly Four-Fifths of White Evangelicals Say T...   \n",
       "...      ...                                                ...   \n",
       "8325    8325  When a Basic Laptop Will Do: Our Top Budget Pi...   \n",
       "8973    8973  Leader of a Ku Klux Klan Group Is Found Dead i...   \n",
       "6242    6242  Economic Breadth Is Significantly Deterioratin...   \n",
       "9319    9319  Keith Lamont Scott Was Killed by Two Gunshot W...   \n",
       "2352    2352  A Fatality Forces Tesla to Confront Its Limits...   \n",
       "\n",
       "                                   author  \\\n",
       "17038                             Pam Key   \n",
       "7623                          Glenn Kenny   \n",
       "1647          Ben Hubbard and Hwaida Saad   \n",
       "4727   Mitch Smith and Richard Pérez-Peña   \n",
       "13379                    Laurie Goodstein   \n",
       "...                                   ...   \n",
       "8325                         Damon Darlin   \n",
       "8973                           Liam Stack   \n",
       "6242                                  IWB   \n",
       "9319                        Niraj Chokshi   \n",
       "2352                          Bill Vlasic   \n",
       "\n",
       "                                                    text  \\\n",
       "17038  Wednesday on CNN addressing media reports that...   \n",
       "7623   It was early in March 2015, the filmmaker Ava ...   \n",
       "1647   BEIRUT, Lebanon  —   After months of fierce bo...   \n",
       "4727   ST. CLOUD, Minn.  —   The man who the police s...   \n",
       "13379  Nearly   of white evangelical voters plan to c...   \n",
       "...                                                  ...   \n",
       "8325   Ever since the advent of the iPhone and iPad, ...   \n",
       "8973   Frank Ancona, the professed leader of the Trad...   \n",
       "6242   Economic Breadth Is Significantly Deterioratin...   \n",
       "9319   A    black man whose fatal shooting by the pol...   \n",
       "2352   DETROIT  —   As the   and chief executive of T...   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "17038  wednesday cnn addressing medium report turned ...   \n",
       "7623   early march filmmaker ava duvernay recalled re...   \n",
       "1647   beirut lebanon month fierce bombardment failed...   \n",
       "4727   st cloud minn man police say stabbed people ma...   \n",
       "13379  nearly white evangelical voter plan cast ballo...   \n",
       "...                                                  ...   \n",
       "8325   ever since advent iphone ipad people using lap...   \n",
       "8973   frank ancona professed leader traditionalist a...   \n",
       "6242   economic breadth significantly deteriorating u...   \n",
       "9319   black man whose fatal shooting police septembe...   \n",
       "2352   detroit chief executive tesla motor technology...   \n",
       "\n",
       "                                             clean_title  \\\n",
       "17038  mccain dont know trumprussia dossier credible ...   \n",
       "7623   netflix cast wider net original documentary ne...   \n",
       "1647   aleppo evacuation effort restarts assad call h...   \n",
       "4727   friend say minnesota attacker normal american ...   \n",
       "13379  nearly fourfifths white evangelicals say theyl...   \n",
       "...                                                  ...   \n",
       "8325          basic laptop top budget pick new york time   \n",
       "8973   leader ku klux klan group found dead missouri ...   \n",
       "6242      economic breadth significantly deteriorating u   \n",
       "9319   keith lamont scott killed two gunshot wound fa...   \n",
       "2352   fatality force tesla confront limit new york time   \n",
       "\n",
       "                                                  tokens  neg_cleaned_text  \\\n",
       "17038  [wednesday, cnn, addressing, medium, report, t...             0.071   \n",
       "7623   [early, march, filmmaker, ava, duvernay, recal...             0.047   \n",
       "1647   [beirut, lebanon, month, fierce, bombardment, ...             0.196   \n",
       "4727   [st, cloud, minn, man, police, say, stabbed, p...             0.137   \n",
       "13379  [nearly, white, evangelical, voter, plan, cast...             0.036   \n",
       "...                                                  ...               ...   \n",
       "8325   [ever, since, advent, iphone, ipad, people, us...             0.041   \n",
       "8973   [frank, ancona, professed, leader, traditional...             0.200   \n",
       "6242   [economic, breadth, significantly, deteriorati...             0.109   \n",
       "9319   [black, man, whose, fatal, shooting, police, s...             0.212   \n",
       "2352   [detroit, chief, executive, tesla, motor, tech...             0.100   \n",
       "\n",
       "       neu_cleaned_text  pos_cleaned_text  compound  neg_clean_title  \\\n",
       "17038             0.871             0.057   -0.4348            0.000   \n",
       "7623              0.781             0.172    0.9965            0.000   \n",
       "1647              0.706             0.097   -0.9981            0.000   \n",
       "4727              0.742             0.122   -0.9646            0.248   \n",
       "13379             0.773             0.191    0.9958            0.000   \n",
       "...                 ...               ...       ...              ...   \n",
       "8325              0.719             0.240    0.9979            0.000   \n",
       "8973              0.701             0.099   -0.9968            0.301   \n",
       "6242              0.709             0.182    0.8470            0.000   \n",
       "9319              0.737             0.051   -0.9806            0.273   \n",
       "2352              0.780             0.120    0.7666            0.508   \n",
       "\n",
       "       neu_clean_title  pos_clean_title  compound  percent_stop  \n",
       "17038            1.000            0.000    0.0000      0.000000  \n",
       "7623             0.777            0.223    0.3182      0.015123  \n",
       "1647             1.000            0.000    0.0000      0.000000  \n",
       "4727             0.537            0.215   -0.1280      0.002963  \n",
       "13379            1.000            0.000    0.0000      0.005102  \n",
       "...                ...              ...       ...           ...  \n",
       "8325             0.795            0.205    0.2023      0.005525  \n",
       "8973             0.699            0.000   -0.6486      0.000000  \n",
       "6242             1.000            0.000    0.0000      0.007937  \n",
       "9319             0.727            0.000   -0.6705      0.000000  \n",
       "2352             0.492            0.000   -0.7351      0.000000  \n",
       "\n",
       "[6810 rows x 16 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['tokens'] = X_test['cleaned_text'].apply(lambda x: x.split())\n",
    "x_test_combined = sentiment_maker(X_test, \"cleaned_text\")\n",
    "x_test_combined = sentiment_maker(x_test_combined, \"clean_title\")\n",
    "x_test_combined['percent_stop'] = x_test_combined['tokens'].apply(count_stop_words)\n",
    "x_test_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atin3\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<13826x104134 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3673094 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=True)\n",
    "tfidf_matrix = vectorizer.fit_transform(X_train['cleaned_text'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=X_train.index)\n",
    "X_test_tfidf = vectorizer.transform(X_test['cleaned_text'])\n",
    "\n",
    "# Extract numerical sentiment & stop-word features\n",
    "X_train_extra = np.array(X_vec[[\"pos_cleaned_text\", \"neu_cleaned_text\", \"neg_cleaned_text\", \"percent_stop\"]])\n",
    "X_test_extra = np.array(x_test_combined[[\"pos_cleaned_text\", \"neu_cleaned_text\", \"neg_cleaned_text\", \"percent_stop\"]])\n",
    "\n",
    "# Stack TF-IDF matrix with additional features\n",
    "X_train_final = hstack([tfidf_matrix, X_train_extra])\n",
    "X_test_final = hstack([X_test_tfidf, X_test_extra])\n",
    "X_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.708223201174743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.48      0.45      1714\n",
      "           1       0.82      0.78      0.80      5096\n",
      "\n",
      "    accuracy                           0.71      6810\n",
      "   macro avg       0.62      0.63      0.63      6810\n",
      "weighted avg       0.72      0.71      0.71      6810\n",
      "\n",
      "y_test distribution: [1714 5096]\n",
      "y_pred distribution: [1925 4885]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "# Predict on test set\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_final, y_train)\n",
    "y_pred_nb = model.predict(X_test_final)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"y_test distribution:\", np.bincount(y_test))\n",
    "print(\"y_pred distribution:\", np.bincount(y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning(Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atin3\\AppData\\Local\\Temp\\ipykernel_14140\\3812275071.py:6: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote = mode(predictions, axis=0).mode[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6807635829662262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.79      0.56      1714\n",
      "           1       0.90      0.64      0.75      5096\n",
      "\n",
      "    accuracy                           0.68      6810\n",
      "   macro avg       0.66      0.72      0.65      6810\n",
      "weighted avg       0.78      0.68      0.70      6810\n",
      "\n",
      "y_test distribution: [1714 5096]\n",
      "y_pred distribution: [3172 3638]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "predictions = np.vstack((y_pred_logistic, y_pred_svm, y_pred_nb))\n",
    "\n",
    "majority_vote = mode(predictions, axis=0).mode[0]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, majority_vote))\n",
    "print(classification_report(y_test, majority_vote))\n",
    "print(\"y_test distribution:\", np.bincount(y_test))\n",
    "print(\"y_pred distribution:\", np.bincount(majority_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
